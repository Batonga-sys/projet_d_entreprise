{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTATION DES BIBLIOTHEQUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from neattext import functions\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAITEMENT DES DONNEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation du jeu de données\n",
    "dataset = pd.read_csv(\"./dataset_police_nationale.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de récupération des dates d'au plus 2 ans\n",
    "def select_dates():\n",
    "    dataset[\"Dates\"] = dataset[\"Dates\"].apply(lambda date: date.replace(\"un\", \"1\"))\n",
    "    indices_to_keep = []\n",
    "    indices_to_remove = []\n",
    "    \n",
    "    for indice in range(len(dataset[\"Dates\"])):\n",
    "        if \"jours\" in dataset[\"Dates\"][indice] or \"semaine\" in dataset[\"Dates\"][indice] or \"mois\" in dataset[\"Dates\"][indice]:\n",
    "            indices_to_keep.append(indice)\n",
    "        elif \"1 an\" in dataset[\"Dates\"][indice] or \"2 ans\" in dataset[\"Dates\"][indice]:\n",
    "            indices_to_keep.append(indice)\n",
    "            \n",
    "    for elt in range(len(dataset)):\n",
    "        if elt not in indices_to_keep:\n",
    "            indices_to_remove.append(elt)\n",
    "            \n",
    "    dataset.drop(indices_to_remove, axis=0, inplace=True)\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de nettoyage des données\n",
    "def clean_avis(avis):\n",
    "    stop_words = set(stopwords.words('french')) # Charger les mots vides (stop words)\n",
    "    cleaned_avis = functions.remove_emojis(avis)\n",
    "    cleaned_avis = re.sub(r'\\W+', ' ', cleaned_avis.lower())  # Supprimer les caractères non alphabétiques et convertir en minuscules\n",
    "    cleaned_avis = re.sub(r'\\d+', '', cleaned_avis)  # Supprimer les chiffres\n",
    "    cleaned_avis = ' '.join([word for word in cleaned_avis.split() if word not in stop_words])  # Supprimer les mots vides\n",
    "    return cleaned_avis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les données dont les dates n'excèdent pas 2 ans\n",
    "dataset = select_dates()\n",
    "\n",
    "# Nettoyer les avis du jeu de données\n",
    "dataset[\"Processing\"] = dataset[\"Avis\"].apply(clean_avis)\n",
    "\n",
    "# Tokenisation des avis\n",
    "dataset[\"Processing\"] = dataset[\"Processing\"].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une short list de mots négatifs\n",
    "def create_short_lists():\n",
    "    # Initialiser l'analyseur de sentiments\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Liste pour stocker les mots très négatifs\n",
    "    positive_short_list = []\n",
    "    negative_short_list = []\n",
    "    neutral_short_list = []\n",
    "\n",
    "    # Parcourir les mots\n",
    "    for indice in range(len(dataset[\"Processing\"])):\n",
    "        for word in dataset[\"Processing\"][indice]:\n",
    "            # Calculer la polarité du mot\n",
    "            neutral_polarity = sia.polarity_scores(word)['neu']\n",
    "            # Sélectionner les mots ayant une polarité négative élevée\n",
    "            if neutral_polarity > 0:\n",
    "                neutral_short_list.append(word)\n",
    "            elif neutral_polarity == 0:\n",
    "                negative_polarity = sia.polarity_scores(word)['neg']\n",
    "                if negative_polarity > 0:\n",
    "                    negative_short_list.append(word)\n",
    "                else:\n",
    "                    positive_short_list.append(word)\n",
    "                \n",
    "    return list(set(negative_short_list)), list(set(neutral_short_list)), list(set(positive_short_list))\n",
    "\n",
    "words_short_list = create_short_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words_short_list[0]:\n",
    "    if len(word) == 2 or word == \"tout\":\n",
    "        words_short_list[0].remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribuer des labels aux avis\n",
    "labels_list = []\n",
    "for word in dataset[\"Processing\"].apply(lambda x: ' '.join(x)):\n",
    "    # Recherche de mots clés négatifs dans le commentaire\n",
    "    negative_word_found = re.findall(r'\\b(?:{})\\b'.format('|'.join(words_short_list[0])), word)\n",
    "    neutral_word_found = re.findall(r'\\b(?:{})\\b'.format('|'.join(words_short_list[1])), word)\n",
    "    positive_word_found = re.findall(r'\\b(?:{})\\b'.format('|'.join(words_short_list[2])), word)\n",
    "    \n",
    "    if negative_word_found:\n",
    "        labels_list.append(\"Négatif\")\n",
    "    elif neutral_word_found:\n",
    "        labels_list.append(\"Neutre\")\n",
    "    else:\n",
    "        labels_list.append(\"Positif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Labels\"] = labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_to_remove = []\n",
    "for elt in range(len(dataset[\"Labels\"])):\n",
    "    if dataset[\"Labels\"][elt] == \"Positif\":\n",
    "        indice_to_remove.append(elt)\n",
    "        \n",
    "# Supprimer les commentaires contenant que les emojis\n",
    "dataset.drop(indice_to_remove, axis=0, inplace=True)\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des commentaires négatifs\n",
    "indices_to_keep = []\n",
    "indices_to_remove = []\n",
    "\n",
    "for indice in range(len(dataset[\"Labels\"])):\n",
    "    if dataset[\"Labels\"][indice] == \"Négatif\":\n",
    "        indices_to_keep.append(indice)\n",
    "        \n",
    "for index in range(len(dataset[\"Labels\"])):\n",
    "    if index not in indices_to_keep:\n",
    "        indices_to_remove.append(index)\n",
    "        \n",
    "dataset.drop(indices_to_remove, axis=0, inplace=True)\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
